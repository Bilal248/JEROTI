{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce727cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.ModelTraining  import *\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce4a63ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agreement between Isolation Forest and DBSCAN: 99.1781%\n",
      "\n",
      "Relative Confusion Matrix between models:\n",
      "DBSCAN               0    1\n",
      "IsolationForest            \n",
      "0                42297  109\n",
      "1                  243  177\n"
     ]
    }
   ],
   "source": [
    "df['dbscan_label'] = df['dbscan_anomaly']\n",
    "\n",
    "\n",
    "df['iso_label'] = df['anomaly'].map({-1: 1, 1: 0})\n",
    "\n",
    "\n",
    "agreement = (df['iso_label'] == df['dbscan_label']).mean()\n",
    "\n",
    "print(f\"Agreement between Isolation Forest and DBSCAN: {agreement:.4%}\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "conf_matrix = pd.crosstab(\n",
    "    df['iso_label'], \n",
    "    df['dbscan_label'], \n",
    "    rownames=['IsolationForest'], \n",
    "    colnames=['DBSCAN']\n",
    ")\n",
    "print(\"\\nRelative Confusion Matrix between models:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "992427b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "F1 Score: 0.5014\n",
      "Precision: 0.6189\n",
      "Recall: 0.4214\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00     42406\n",
      "           1       0.62      0.42      0.50       420\n",
      "\n",
      "    accuracy                           0.99     42826\n",
      "   macro avg       0.81      0.71      0.75     42826\n",
      "weighted avg       0.99      0.99      0.99     42826\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_true = df['iso_label']      \n",
    "y_pred = df['dbscan_label']    \n",
    "\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "\n",
    "print(f\"\\nF1 Score: {f1:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1053d5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ada_label'] = ada.predict(X_pca)\n",
    "df['ada_prob'] = ada.predict_proba(X_pca)[:, 1]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ac7b60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ada_label'].unique()\n",
    "df['ada_label'] = df['ada_label'].map({-1: 1, 1: 0})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a28dedc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.2817\n",
      "Precision: 0.1896\n",
      "Recall: 0.5476\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41423   983]\n",
      " [  190   230]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     42406\n",
      "           1       0.19      0.55      0.28       420\n",
      "\n",
      "    accuracy                           0.97     42826\n",
      "   macro avg       0.59      0.76      0.63     42826\n",
      "weighted avg       0.99      0.97      0.98     42826\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = df['iso_label']\n",
    "y_pred = df['ada_label']\n",
    "\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e7e12e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.2628\n",
      "Precision: 0.1624\n",
      "Recall: 0.6888\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41524  1016]\n",
      " [   89   197]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     42540\n",
      "           1       0.16      0.69      0.26       286\n",
      "\n",
      "    accuracy                           0.97     42826\n",
      "   macro avg       0.58      0.83      0.62     42826\n",
      "weighted avg       0.99      0.97      0.98     42826\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = df['dbscan_label']\n",
    "y_pred = df['ada_label']\n",
    "\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JEROTI (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
